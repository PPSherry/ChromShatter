{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取SV断点图\n",
    "每一页有四个SV断点图，其坐标确定方法为：x坐标以染色体区间的两端为准，y坐标以上下两条界线为准;  \n",
    "每一页内的顺序为，先左上 -> 右上 -> 左下 -> 右下\n",
    "* 此前我用原PDF文件的子集形成一个新的PDF文件，来确定SV图像的坐标；但当应用于原PDF文件时，SV图像的坐标却发生了变化\n",
    "* 建议：根据原PDF文件转换后的图片，确定SV坐标，更改coordinates以及w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# 将PDF文件转换为图片\n",
    "def convert_pdf_to_images(pdf_path, output_dir, dpi=300):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "    for i, page in enumerate(pages):\n",
    "        page_filename = f\"{output_dir}/page_{i+1}.png\"\n",
    "        page.save(page_filename, \"PNG\")\n",
    "        print(f\"Saved page as image: {page_filename}\")\n",
    "\n",
    "# 根据手动确定的SV图像坐标，为每一页切割出4个SV断点图\n",
    "def extract_sv_image_from_page(image_path, output_dir, coordinates):\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    for idx, (x, y) in enumerate(coordinates):\n",
    "        w, h = 1050, 380 # 切割的宽度和高度\n",
    "        cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        # 检查裁剪后的图像尺寸，如果不是1050x320则进行尺寸调整\n",
    "        target_width, target_height = 1050, 320\n",
    "        if cropped_img.shape[1] != target_width or cropped_img.shape[0] != target_height:\n",
    "            print(f\"调整图像尺寸从 {cropped_img.shape[1]}x{cropped_img.shape[0]} 到 {target_width}x{target_height}\")\n",
    "            cropped_img = cv2.resize(cropped_img, (target_width, target_height))\n",
    "\n",
    "        output_filename = f\"{output_dir}/{os.path.splitext(os.path.basename(image_path))[0]}_part_{idx+1}.png\"\n",
    "        cv2.imwrite(output_filename, cropped_img)\n",
    "        print(f\"Saved cropped SV image: {output_filename}\")\n",
    "\n",
    "\n",
    "# 设置文件路径\n",
    "pdf_path = \"/Users/xurui/back_up_unit/天津大学文件/本科毕设相关/Article/ShatterSeek_data/NG_Supplementary_db_4.pdf\"\n",
    "page_output_dir = \"/Volumes/T7-shield/CS-Bachelor-Thesis/CNN_model/pdf_convert\"\n",
    "image_output_dir = \"/Volumes/T7-shield/CS-Bachelor-Thesis/CNN_model/SV_graph.PCAWG/SV_graph.dataset4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将PDF文件转换为图片\n",
    "convert_pdf_to_images(pdf_path, page_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为每一个页面切除4个SV图像  \n",
    "基于每一页的dppi为300的情况\n",
    "* high confidence 坐标 h,w = 1050, 320  \n",
    "coordinates = [\n",
    "    (71, 150),\n",
    "    (1290, 150),\n",
    "    (71, 2005),\n",
    "    (1290, 2005)\n",
    "]\n",
    "* low confidence 坐标 h,w = 1050, 320  \n",
    "coordinates = [\n",
    "    (90，146),\n",
    "    (1290, 146),\n",
    "    (90, 1972),\n",
    "    (1290, 1972)\n",
    "]\n",
    "* dataset 3 坐标 h, w = 1050, 370  \n",
    "coordinates = [\n",
    "    (85, 170),\n",
    "    (1300 ,170),\n",
    "    (85, 1980),\n",
    "    (1300, 1980)\n",
    "]\n",
    "* dataset 4 坐标 h, w = 1050, 380\n",
    "coordinates = [\n",
    "    (80, 170),\n",
    "    (1290, 170),\n",
    "    (80, 2028),\n",
    "    (1290, 2028)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用脚本 get_coordinates.py 获取坐标\n",
    "coordinates = [\n",
    "    (80, 170),\n",
    "    (1290, 170),\n",
    "    (80, 2028),\n",
    "    (1290, 2028)\n",
    "]\n",
    "\n",
    "if not os.path.exists(image_output_dir):\n",
    "    os.makedirs(image_output_dir)\n",
    "\n",
    "for page_file in os.listdir(page_output_dir):\n",
    "    if page_file.endswith(\".png\"):\n",
    "        page_path = os.path.join(page_output_dir, page_file)\n",
    "        extract_sv_image_from_page(page_path, image_output_dir, coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取文本信息\n",
    "使用 extract_text_info_from_PDF.py 实现，提取样本名，Position, Oscillating CN （2 and 3 states） CN segments\n",
    "* 需要注意的地方：\n",
    "    * 为了避免OCR图片带来的错误，选择直接从PDF文件中提取文本部分\n",
    "    * 页面之间四个染色体破裂事件的分布，并不是完全一样的（_都是左上角的事件，文本部分坐标kennel有细微差别_），所以划分区间的时候需要注意，可以稍微大一些\n",
    "    * 算法中匹配样本名时发现，PCAWG原始表格中短横线 (-) 有多种类型，需要额外注意！\n",
    "* 实现的算法：\n",
    "    1. 先将每一页的文本都提取出来，包括文本和坐标（放在字典里），坐标用于划分到不同的事件中\n",
    "    2. 用其他脚本，手动确定每个事件的大概范围，制作文字部分区间\n",
    "    3. 同一文字区间的文本视为同一个事件的信息\n",
    "    4. 匹配所需要的信息（例如样本名，POS），方法比较复杂，不同信息提取方式也略有不同\n",
    "        * 样本名是使用PCAWG提供表格中 donor_idx 进行匹配  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证提取的文本信息中有无缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于提取PDF时缺失的值，根据PCAWG的表格中的数据进行补充  \n",
    "使用 *detect_missing_values.py* 脚本进行判断 \n",
    "PCAWG表格和我们提取的表格之间，列的映射关系：\n",
    "* CN segments -> cn_segments\n",
    "* Nb. oscillating.CN -> CN_2_states\n",
    "* Nb. oscillaring CN 3 states -> CN_3_states\n",
    "* Chr + Start + End -> position\n",
    "* donor_idx -> sample_name\n",
    "    * 提取PDF中 sample_name 时出现了 (-) 不匹配PCAWG表格中 (-) 的问题，故提取信息的表格中使用的sample_name不是直接从PDF中提取的，而是使用PCAWG表格中对应的 donor_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### high confidence results \n",
    "Total rows in input file: 1136  \n",
    "Rows with missing values: 21  \n",
    "Missing values by column:  \n",
    "  position: 21 missing values  \n",
    "  \n",
    "即只存在position列没有提取成功的情况 [经确认，是正则匹配时忽略了X染色体的情况，修改后已无缺失值]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low confidence results\n",
    "经过检测没有缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 3 results\n",
    "Total rows in input file: 816  \n",
    "Rows with missing values: 216 \n",
    "Missing values by column:  \n",
    "  position: 216 missing values  \n",
    "  oscillating_cn_2&3_states: 5 missing values  \n",
    "  cn_segments: 216 missing values  \n",
    "经过检查，检测不成功的样本是 position 显示为染色体号，没有区间；并且 CNV 信息也只有在两个状态震荡的值，后续会被舍弃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 4 Results\n",
    "没有缺失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用PCAWG人工审查结果打标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCAWG中有注释的条目只有两种类型: False negative; manually included 和 False positive; manually removed  \n",
    "* High Confidence 中没有任何匹配项\n",
    "* Low Confidence 中没有任何匹配\n",
    "* Dataset3 有162个匹配项 -- 已经去除了 missing values 的情况\n",
    "    * 161 个 False Positive\n",
    "    * 1 个 False Negative (C0052 chr6) \n",
    "* Dataset4 有两个匹配项\n",
    "    * 2个 False negative; manually included\n",
    "    * donor_idx: 17dffffc-65d6-4209-9075-18a441001f0f Chr: 12\n",
    "    * donor_idx: c184c3ca-7ad3-4202-b108-cb9fd5f5d947 Chr: 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 第一步: 处理combined_events.tsv ===\n",
      "位置拆分完成，成功: 264, 失败: 0\n",
      "拆分结果已保存到 /Volumes/T7-shield/CS-Bachelor-Thesis/CNN_model/SV_graph.PCAWG/CNV_info_from_PDF/dataset4_info/combined_events_split.tsv\n",
      "\n",
      "=== 第二步: 查找匹配记录 ===\n",
      "\n",
      "匹配记录 #1:\n",
      "  Abnormal记录 (行 69):\n",
      "    donor_idx: 17dffffc-65d6-4209-9075-18a441001f0f\n",
      "    Chr: 12\n",
      "    Start: 57486350.0\n",
      "    End: 69728850.0\n",
      "    comment: False negative; manually included\n",
      "  匹配的Combined记录:\n",
      "    sample_name: 17dffffc-65d6-4209-9075-18a441001f0f\n",
      "    position: 12:57486350−69728850\n",
      "    Chr: 12\n",
      "    Start: 57486350\n",
      "    End: 69728850\n",
      "    oscillating_cn: 3, 6\n",
      "    cn_segments: 39\n",
      "\n",
      "匹配记录 #2:\n",
      "  Abnormal记录 (行 76):\n",
      "    donor_idx: c184c3ca-7ad3-4202-b108-cb9fd5f5d947\n",
      "    Chr: 12\n",
      "    Start: 57802243.0\n",
      "    End: 59971140.0\n",
      "    comment: False negative; manually included\n",
      "  匹配的Combined记录:\n",
      "    sample_name: c184c3ca-7ad3-4202-b108-cb9fd5f5d947\n",
      "    position: 12:57802243−59971140\n",
      "    Chr: 12\n",
      "    Start: 57802243\n",
      "    End: 59971140\n",
      "    oscillating_cn: 3, 5\n",
      "    cn_segments: 48\n",
      "\n",
      "总计找到 2 条匹配记录\n",
      "\n",
      "Comment频率统计:\n",
      "  'False negative; manually included': 2次 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def split_position(position_str):\n",
    "    \"\"\"\n",
    "    将位置字符串(Chr:Start-End)拆分为三个组件\n",
    "    \"\"\"\n",
    "    if not isinstance(position_str, str) or not position_str:\n",
    "        return None, None, None\n",
    "        \n",
    "    # 使用正则表达式匹配位置格式\n",
    "    match = re.match(r'([0-9XY]+):(\\d+)[−-](\\d+)', position_str)\n",
    "    if match:\n",
    "        chr_val = match.group(1)\n",
    "        start_val = int(match.group(2))\n",
    "        end_val = int(match.group(3))\n",
    "        return chr_val, start_val, end_val\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def process_combined_events(tsv_path):\n",
    "    \"\"\"\n",
    "    处理combined_events.tsv文件，拆分position列\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取TSV文件\n",
    "        df = pd.read_csv(tsv_path, sep='\\t')\n",
    "        \n",
    "        # 检查position列是否存在\n",
    "        if 'position' not in df.columns:\n",
    "            print(f\"错误: {tsv_path}中没有'position'列\")\n",
    "            return None\n",
    "            \n",
    "        # 创建新列存储拆分结果\n",
    "        df['Chr'] = None\n",
    "        df['Start'] = None\n",
    "        df['End'] = None\n",
    "        \n",
    "        # 拆分每一行的position\n",
    "        failure_count = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            chr_val, start_val, end_val = split_position(row['position'])\n",
    "            if chr_val is None:\n",
    "                failure_count += 1\n",
    "                print(f\"警告: 无法拆分position值 '{row['position']}' (行 {idx+2})\")\n",
    "            else:\n",
    "                df.at[idx, 'Chr'] = chr_val\n",
    "                df.at[idx, 'Start'] = start_val\n",
    "                df.at[idx, 'End'] = end_val\n",
    "        \n",
    "        print(f\"位置拆分完成，成功: {len(df) - failure_count}, 失败: {failure_count}\")\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"处理{tsv_path}时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_matching_records(combined_df, abnormal_path):\n",
    "    \"\"\"\n",
    "    查找abnormal_calls_from_PCAWG.tsv中与combined_events.tsv匹配的记录\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取abnormal文件\n",
    "        abnormal_df = pd.read_csv(abnormal_path, sep='\\t')\n",
    "        \n",
    "        # 检查必要的列是否存在\n",
    "        required_cols = ['donor_idx', 'Chr', 'Start', 'End']\n",
    "        for col in required_cols:\n",
    "            if col not in abnormal_df.columns:\n",
    "                print(f\"错误: {abnormal_path}中缺少'{col}'列\")\n",
    "                return\n",
    "        \n",
    "        # 确保数据类型一致\n",
    "        combined_df['Chr'] = combined_df['Chr'].astype(str)\n",
    "        combined_df['Start'] = pd.to_numeric(combined_df['Start'], errors='coerce')\n",
    "        combined_df['End'] = pd.to_numeric(combined_df['End'], errors='coerce')\n",
    "        \n",
    "        abnormal_df['Chr'] = abnormal_df['Chr'].astype(str)\n",
    "        abnormal_df['Start'] = pd.to_numeric(abnormal_df['Start'], errors='coerce')\n",
    "        abnormal_df['End'] = pd.to_numeric(abnormal_df['End'], errors='coerce')\n",
    "        \n",
    "        # 创建字典用于统计comment频率\n",
    "        comment_counts = {}\n",
    "        \n",
    "        # 遍历abnormal文件中的每一行\n",
    "        match_count = 0\n",
    "        for idx, abnormal_row in abnormal_df.iterrows():\n",
    "            # 查找匹配记录\n",
    "            matches = combined_df[\n",
    "                (combined_df['sample_name'] == abnormal_row['donor_idx']) &\n",
    "                (combined_df['Chr'] == abnormal_row['Chr']) &\n",
    "                (combined_df['Start'] == abnormal_row['Start']) &\n",
    "                (combined_df['End'] == abnormal_row['End'])\n",
    "            ]\n",
    "            \n",
    "            if len(matches) > 0:\n",
    "                match_count += 1\n",
    "                print(f\"\\n匹配记录 #{match_count}:\")\n",
    "                print(f\"  Abnormal记录 (行 {idx+2}):\")\n",
    "                print(f\"    donor_idx: {abnormal_row['donor_idx']}\")\n",
    "                print(f\"    Chr: {abnormal_row['Chr']}\")\n",
    "                print(f\"    Start: {abnormal_row['Start']}\")\n",
    "                print(f\"    End: {abnormal_row['End']}\")\n",
    "                \n",
    "                # 打印comment列(如果存在)\n",
    "                if 'comment' in abnormal_row:\n",
    "                    comment = str(abnormal_row['comment'])\n",
    "                    print(f\"    comment: {comment}\")\n",
    "                    \n",
    "                    # 统计comment频率\n",
    "                    if comment in comment_counts:\n",
    "                        comment_counts[comment] += 1\n",
    "                    else:\n",
    "                        comment_counts[comment] = 1\n",
    "                \n",
    "                for _, match in matches.iterrows():\n",
    "                    print(f\"  匹配的Combined记录:\")\n",
    "                    print(f\"    sample_name: {match['sample_name']}\")\n",
    "                    print(f\"    position: {match['position']}\")\n",
    "                    print(f\"    Chr: {match['Chr']}\")\n",
    "                    print(f\"    Start: {match['Start']}\")\n",
    "                    print(f\"    End: {match['End']}\")\n",
    "                    if 'oscillating_cn_2&3_states' in match:\n",
    "                        print(f\"    oscillating_cn: {match['oscillating_cn_2&3_states']}\")\n",
    "                    if 'cn_segments' in match:\n",
    "                        print(f\"    cn_segments: {match['cn_segments']}\")\n",
    "        \n",
    "        print(f\"\\n总计找到 {match_count} 条匹配记录\")\n",
    "        \n",
    "        # 打印comment频率统计\n",
    "        if comment_counts:\n",
    "            print(\"\\nComment频率统计:\")\n",
    "            # 按频率降序排序\n",
    "            sorted_comments = sorted(comment_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "            for comment, count in sorted_comments:\n",
    "                print(f\"  '{comment}': {count}次 ({count/match_count*100:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理{abnormal_path}时出错: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    # 设置文件路径\n",
    "    base_dir = \"/Volumes/T7-shield/CS-Bachelor-Thesis/CNN_model\"\n",
    "    combined_path = os.path.join(base_dir, \"SV_graph.PCAWG/CNV_info_from_PDF/dataset4_info/combined_events.tsv\")\n",
    "    abnormal_path = os.path.join(base_dir, \"PCAWG-related/abnormal_calls_from_PCAWG.tsv\")\n",
    "    output_path = os.path.join(base_dir, \"SV_graph.PCAWG/CNV_info_from_PDF/dataset4_info/combined_events_split.tsv\")\n",
    "\n",
    "    # 检查文件是否存在\n",
    "    for path in [combined_path, abnormal_path]:\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"错误: 文件 {path} 不存在\")\n",
    "            return\n",
    "    \n",
    "    # 第一步: 处理combined_events.tsv，拆分position列\n",
    "    print(\"=== 第一步: 处理combined_events.tsv ===\")\n",
    "    combined_df = process_combined_events(combined_path)\n",
    "    if combined_df is None:\n",
    "        return\n",
    "    \n",
    "    # 保存拆分后的结果（可选）\n",
    "    combined_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"拆分结果已保存到 {output_path}\")\n",
    "    \n",
    "    # 第二步: 查找匹配记录\n",
    "    print(\"\\n=== 第二步: 查找匹配记录 ===\")\n",
    "    find_matching_records(combined_df, abnormal_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdf_extract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
